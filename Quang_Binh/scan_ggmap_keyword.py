import time
import csv
import os
import re
from playwright.sync_api import sync_playwright
from datetime import datetime

# ======================
# ğŸ” NHáº¬P Tá»ª CONSOLE
# ======================
location_input = input("ğŸ“ Nháº­p location (lat,lng): ").strip()
keyword = input("ğŸ” Nháº­p keyword tÃ¬m kiáº¿m: ").strip()

if not location_input or not keyword:
    print("âŒ Location vÃ  Keyword khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng")
    exit()

# ======================
# ğŸ§­ PARSE lat,lng
# ======================
def parse_latlng(text):
    match = re.match(r"\s*(-?\d+(\.\d+)?)\s*,\s*(-?\d+(\.\d+)?)\s*", text)
    if not match:
        return None, None
    return match.group(1), match.group(3)

center_lat, center_lng = parse_latlng(location_input)
if not center_lat:
    print("âŒ Location pháº£i Ä‘Ãºng dáº¡ng lat,lng (vd: 10.2435,106.3752)")
    exit()

# ======================
# ğŸ“ FILE CSV THEO KEYWORD
# ======================
def sanitize_filename(text):
    text = text.strip().lower()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"\s+", "_", text)
    return text

safe_keyword = sanitize_filename(keyword)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
# Láº¥y thÆ° má»¥c chá»©a file .py
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

output_file = os.path.join(
    BASE_DIR,
    f"googlemaps_{safe_keyword}_{timestamp}.csv"
)

# ======================
# ğŸ§­ TÃCH lat,lng Tá»ª URL
# ======================
def extract_latlng_from_url(url):
    m1 = re.search(r"@(-?\d+\.\d+),(-?\d+\.\d+)", url)
    if m1:
        return m1.group(1), m1.group(2)
    m2 = re.search(r"!3d(-?\d+\.\d+)!4d(-?\d+\.\d+)", url)
    if m2:
        return m2.group(1), m2.group(2)
    return "", ""

# ======================
# ğŸ’¾ AUTOSAVE CSV
# ======================
def save_to_csv(data):
    file_exists = os.path.isfile(output_file)
    with open(output_file, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=[
                "center_lat","center_lng","keyword",
                "name","address","phone","website","open_hours",
                "lat","lng","url"
            ]
        )
        if not file_exists:
            writer.writeheader()
        writer.writerow(data)
        f.flush()

# ======================
# ğŸŒ€ SCROLL DANH SÃCH
# ======================
def scroll_results(page, max_round=40):
    feed = page.query_selector("div[role='feed']")
    if not feed:
        return False
    last_height = 0
    for _ in range(max_round):
        page.evaluate("(el) => el.scrollBy(0, el.scrollHeight)", feed)
        time.sleep(1.2)
        new_height = page.evaluate("(el) => el.scrollHeight", feed)
        if new_height == last_height:
            break
        last_height = new_height
    return True

# ======================
# ğŸ“ Láº¤Y Äá»ŠA CHá»ˆ
# ======================
def get_address(page):
    sels = [
        "button[data-item-id='address']",
        "button[aria-label^='Äá»‹a chá»‰']",
        "div[aria-label^='Äá»‹a chá»‰']"
    ]
    for s in sels:
        loc = page.locator(s)
        if loc.count() > 0:
            return loc.first.text_content().strip()
    return "N/A"

def get_phone(page):
    sels = [
        "button[data-item-id^='phone']",
        "button[aria-label^='Sá»‘ Ä‘iá»‡n thoáº¡i']",
        "div[aria-label^='Sá»‘ Ä‘iá»‡n thoáº¡i']"
    ]
    for s in sels:
        loc = page.locator(s)
        if loc.count() > 0:
            return loc.first.text_content().strip()
    return "N/A"

def get_website(page):
    sels = [
        "a[data-item-id='authority']",
        "a[aria-label^='Trang web']",
        "a[aria-label^='Website']"
    ]
    for s in sels:
        loc = page.locator(s)
        if loc.count() > 0:
            return loc.first.get_attribute("href")
    return "N/A"

def get_open_hours(page):
    sels = [
        "div[aria-label^='Giá» má»Ÿ cá»­a']",
        "button[aria-label^='Giá» má»Ÿ cá»­a']",
        "div[aria-label^='Open']"
    ]
    for s in sels:
        loc = page.locator(s)
        if loc.count() > 0:
            return loc.first.text_content().strip()
    return "N/A"

# ======================
# ğŸš€ CRAWL GOOGLE MAPS
# ======================
def crawl_google_maps(center_lat, center_lng, keyword):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, slow_mo=100)
        context = browser.new_context(locale="vi-VN", viewport={"width":1280,"height":800})
        page = context.new_page()

        print(f"ğŸ“ Di chuyá»ƒn tá»›i tá»a Ä‘á»™: {center_lat},{center_lng}")
        page.goto(f"https://www.google.com/maps/@{center_lat},{center_lng},14z?hl=vi", timeout=60000)
        time.sleep(5)

        print(f"ğŸ” TÃ¬m kiáº¿m keyword: {keyword}")
        search_box = page.wait_for_selector("input[role='combobox']", timeout=10000)
        search_box.fill(keyword)
        time.sleep(1)
        search_box.press("Enter")
        time.sleep(6)

        print("ğŸŒ€ Äang kiá»ƒm tra danh sÃ¡ch káº¿t quáº£...")
        feed = page.query_selector("div[role='feed']")
        if feed:
            print("â¡ CÃ³ danh sÃ¡ch, báº¯t Ä‘áº§u cuá»™n...")
            scroll_results(page)
            links = page.eval_on_selector_all(
                "a[href*='/maps/place/']",
                "els => [...new Set(els.map(el => el.href))]"
            )
        else:
            print("â¡ KhÃ´ng cÃ³ danh sÃ¡ch â€“ chá»‰ cÃ³ 1 Ä‘á»‹a Ä‘iá»ƒm")
            links = [page.url]

        print(f"ğŸ“Œ Sá»‘ Ä‘á»‹a Ä‘iá»ƒm láº¥y Ä‘Æ°á»£c: {len(links)}")

        seen = set()
        for idx, link in enumerate(links):
            if link in seen:
                continue
            seen.add(link)
            try:
                page.goto(link, timeout=60000)
                time.sleep(4)

                try:
                    page.wait_for_selector("h1", timeout=5000)
                    name = page.locator("h1").first.text_content()
                except:
                    name = "N/A"

                address = get_address(page)
                phone = get_phone(page)
                website = get_website(page)
                open_hours = get_open_hours(page)
                lat, lng = extract_latlng_from_url(page.url)

                print(f"âœ” {idx+1:03d}: {name}")
                print(f"   ğŸ“ {address}")
                print(f"   ğŸ“ {phone}")
                print(f"   ğŸŒ {website}")
                print(f"   â° {open_hours}")
                print(f"   ğŸ§­ {lat},{lng}")

                save_to_csv({
                    "center_lat": center_lat,
                    "center_lng": center_lng,
                    "keyword": keyword,
                    "name": name,
                    "address": address,
                    "phone": phone,
                    "website": website,
                    "open_hours": open_hours,
                    "lat": lat,
                    "lng": lng,
                    "url": page.url
                })

            except Exception as e:
                print(f"âŒ Lá»—i entry {idx+1}: {e}")

        browser.close()
        print(f"\nâœ… HoÃ n táº¥t â€“ dá»¯ liá»‡u Ä‘Ã£ lÆ°u vÃ o:\n{output_file}")

# ======================
# â–¶ï¸ RUN
# ======================
crawl_google_maps(center_lat, center_lng, keyword)
